{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa115d9-ff60-4228-96a1-fa72020c657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e3c0663b-5473-4c05-8308-5fb99b409113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vdinh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "423bfe44-9397-42d9-8735-6271aeb01434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f3e703c7-5b28-404c-9dff-0453d5540e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open(\"stations_list.txt\", \"r\") as fh:\n",
    "    trainingSet = [] # Training set data\n",
    "    splitNumbers = []\n",
    "    for numbers in fh:\n",
    "        # Split string into list of substrings\n",
    "        splitNumbers = numbers.split()\n",
    "        trainingSet.append(splitNumbers)\n",
    "\n",
    "arr_int = np.array(trainingSet)\n",
    "arr_flat = arr_int.flatten().astype(int)\n",
    "\n",
    "sensor_ids = list(arr_flat)\n",
    "\n",
    "my_list = list(range(402091, 402706)) \n",
    "sensor_ids.extend(my_list)\n",
    "\n",
    "print(sensor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a32c19c9-3394-4d6f-a413-59c42acc7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful!\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Replace these with your PeMS credentials\n",
    "username = \"USERNAME\"\n",
    "password = \"PASSWORD\"\n",
    "\n",
    "# URL for logging into PeMS\n",
    "login_url = \"https://pems.dot.ca.gov/\"\n",
    "\n",
    "# Time parameters (same for all sensor IDs)\n",
    "s_time_id = \"1536537600\"\n",
    "e_time_id = \"1537142340\"\n",
    "s_time_id_f = \"09%2F10%2F2018+00%3A00\"\n",
    "e_time_id_f = \"09%2F16%2F2018+23%3A59\"\n",
    "\n",
    "# Start a session\n",
    "session = requests.Session()\n",
    "\n",
    "# Log in to PeMS\n",
    "login_payload = {\n",
    "    'username': username,\n",
    "    'password': password,\n",
    "    'submit': 'Login'\n",
    "}\n",
    "\n",
    "login_response = session.post(login_url, data=login_payload)\n",
    "\n",
    "if login_response.status_code == 200 and \"logout\" in login_response.text.lower():\n",
    "    print(\"Login successful!\")\n",
    "\n",
    "       sensor_id in sensor_ids:\n",
    "        # Format the data URL with the current sensor ID\n",
    "        data_url = f\"https://pems.dot.ca.gov/?report_form=1&dnode=VDS&content=loops&tab=det_timeseries&export=text&station_id={sensor_id}&s_time_id={s_time_id}&s_time_id_f={s_time_id_f}&e_time_id={e_time_id}&e_time_id_f={e_time_id_f}&tod=all&tod_from=0&tod_to=0&dow_0=on&dow_1=on&dow_2=on&dow_3=on&dow_4=on&dow_5=on&dow_6=on&q=flow&q2=&gn=5min&agg=on&lane1=on&lane2=on&lane3=on&lane4=on\"\n",
    "        \n",
    "        response = session.get(data_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            data_lines = response.text.splitlines()\n",
    "            filtered_data = []\n",
    "\n",
    "            filtered_data.append(data_lines[0])\n",
    "\n",
    "            for line in data_lines[1:]:\n",
    "                columns = line.split(\"\\t\") \n",
    "                columns = [col.strip() for col in columns]\n",
    "                time_column = columns[0]  \n",
    "\n",
    "                try:\n",
    "                    time_part = time_column.split(\" \")[1]  # Get time after the date\n",
    "                    minutes = int(time_part.split(\":\")[1])\n",
    "\n",
    "                    if minutes % 10 == 0:\n",
    "                        filtered_data.append(line)\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping invalid time format in line: {line}\")\n",
    "\n",
    "            filename = f\"occupancy_data_{sensor_id}_filtered.txt\"  # Filename without path\n",
    "\n",
    "            with open(filename, \"w\") as file:\n",
    "                file.write(\"\\n\".join(filtered_data))\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for sensor {sensor_id}. Status code: {response.status_code}\")\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Login failed. Please check your credentials.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760466b-5d47-46b0-bf80-ef723851872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for sensor_id in sensor_ids:\n",
    "    input_file = f\"occupancy_data_{sensor_id}_filtered.txt\"  # Filename without path\n",
    "    df = pd.read_csv(input_file, delimiter=\"\\t\")  # Adjust delimiter if needed\n",
    "    df['5 Minutes'] = pd.to_datetime(df['5 Minutes'], format='%m/%d/%Y %H:%M')\n",
    "    df['Date'] = df['5 Minutes'].dt.date  # Extract the date part\n",
    "    grouped = df.groupby('Date')['Flow (Veh/5 Minutes)'].apply(list).reset_index()\n",
    "    grouped['Flow List'] = grouped['Flow (Veh/5 Minutes)'].apply(lambda x: ' '.join(map(str, x)) + ' ')\n",
    "    grouped = grouped.drop(columns=['Date', 'Flow (Veh/5 Minutes)'])\n",
    "    output_file = f\"filtered_occupancy_data_{sensor_id}_filtered.txt\"  # Filename without path\n",
    "    with open(output_file, 'w') as f:\n",
    "        for row in grouped['Flow List']:\n",
    "            f.write(row + '\\n')\n",
    "\n",
    "# Delete the original file after saving the new file\n",
    "    if os.path.exists(input_file):\n",
    "        os.remove(input_file)\n",
    "        print(f\"Original file {input_file} has been deleted.\")\n",
    "    else:\n",
    "        print(f\"The original file {input_file} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "33d83cb4-b2e9-4538-965a-47f09218dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two files\n",
    "# Create initial seed file\n",
    "seed_Sensor_ID1 = 400000\n",
    "seed_Sensor_ID2 = 400001\n",
    "file1_path = f\"filtered_occupancy_data_{seed_Sensor_ID1}_filtered.txt\"  # Filename without path\n",
    "file2_path = f\"filtered_occupancy_data_{seed_Sensor_ID2}_filtered.txt\"  # Filename without path\n",
    "#    file2_path = f\"filtered_occupancy_data_{sensor_id}_filtered.txt\"  # Filename without path\n",
    "output_path = 'appended_output_seed.txt'\n",
    "\n",
    "# Check if both files exist\n",
    "if not os.path.exists(file1_path):\n",
    "    raise FileNotFoundError(f\"File not found: {file1_path}\")\n",
    "\n",
    "if not os.path.exists(file2_path):\n",
    "    print(f\"File not found: {file2_path}. Skipping file2 and continuing with file1 only.\")\n",
    "    file2_path = None\n",
    "\n",
    "\n",
    "with open(file1_path, 'r') as file1:\n",
    "    lines1 = file1.readlines()\n",
    "\n",
    "\n",
    "if file2_path:\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        lines2 = file2.readlines()\n",
    "\n",
    "\n",
    "    if len(lines1) != len(lines2):\n",
    "        raise ValueError(\"The files do not have the same number of rows.\")\n",
    "\n",
    "    appended_lines = [line1.strip() + line2.strip() + '\\n' for line1, line2 in zip(lines1, lines2)]\n",
    "else:\n",
    "    appended_lines = [line1.strip() + '\\n' for line1 in lines1]\n",
    "    \n",
    "with open(output_path, 'w') as output_file:\n",
    "    output_file.writelines(appended_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c1e76-abfa-4ecc-9d23-28e457473b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_to_remove = [400000, 400001] \n",
    "\n",
    "sensor_ids = [x for x in sensor_ids if x not in numbers_to_remove]\n",
    "\n",
    "print(sensor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeaf319-9b5b-49fc-bc98-7937b29c6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two files\n",
    "for sensor_id in sensor_ids:\n",
    "\n",
    "    file1_path = 'appended_output_seed.txt'\n",
    "    file2_path = f\"filtered_occupancy_data_{sensor_id}_filtered.txt\"  \n",
    "    output_path = 'appended_output_seed.txt'\n",
    "\n",
    "    if not os.path.exists(file1_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file1_path}\")\n",
    "\n",
    "    if not os.path.exists(file2_path):\n",
    "        print(f\"File not found: {file2_path}. Skipping file2 and continuing with file1 only.\")\n",
    "        file2_path = None\n",
    "\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        lines1 = file1.readlines()\n",
    "\n",
    "    if file2_path:\n",
    "        with open(file2_path, 'r') as file2:\n",
    "            lines2 = file2.readlines()\n",
    "\n",
    "        if len(lines1) != len(lines2):\n",
    "            raise ValueError(\"The files do not have the same number of rows.\")\n",
    "\n",
    "        appended_lines = [line1.strip() + line2.strip() + '\\n' for line1, line2 in zip(lines1, lines2)]\n",
    "    else:\n",
    "        appended_lines = [line1.strip() + '\\n' for line1 in lines1]\n",
    "\n",
    "    with open(output_path, 'w') as output_file:\n",
    "        output_file.writelines(appended_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef310429-7782-4bf7-9828-4ee71b1a6fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "with open(\"appended_output_seed.txt\", \"r\") as fh:\n",
    "    testSet = [] # Training set data\n",
    "    splitNumbers = []\n",
    "    # Using a for loop to convert each line of strings into a row of floats\n",
    "    for numbers in fh:\n",
    "        # Split string into list of substrings\n",
    "        splitNumbers = numbers.split()\n",
    "        testSet.append(splitNumbers)\n",
    "\n",
    "# Set in np to reduce computation time in calculations\n",
    "np_Test_Set = np.array(testSet, dtype=np.float64)\n",
    "print(f'Shape of training data: {np_Test_Set.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac49f50-0851-4974-8828-065da0a11ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "tf = TfidfTransformer() # weight word significance\n",
    "nl = Normalizer(copy=False) # scale each data point\n",
    "pl = make_pipeline(tf, nl)\n",
    "Transform_Norm_Test_Data = pl.fit_transform(np_Test_Set)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c226023-5d58-4215-86ab-1f0a7acc2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_data = Transform_Norm_Test_Data.toarray()\n",
    "\n",
    "# Save to a text file\n",
    "output_file = 'self_test.txt'\n",
    "np.savetxt(output_file, dense_data, fmt='%0.5f')  # Save with 5 decimal places\n",
    "\n",
    "print(f\"Transformed data saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
